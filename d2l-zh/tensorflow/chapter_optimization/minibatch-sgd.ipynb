{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0cdeeb2",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 小批量随机梯度下降\n",
    ":label:`sec_minibatch_sgd`\n",
    "\n",
    "到目前为止，我们在基于梯度的学习方法中遇到了两个极端情况：\n",
    " :numref:`sec_gd`中使用完整数据集来计算梯度并更新参数，\n",
    " :numref:`sec_sgd`中一次处理一个训练样本来取得进展。\n",
    "二者各有利弊：每当数据非常相似时，梯度下降并不是非常“数据高效”。\n",
    "而由于CPU和GPU无法充分利用向量化，随机梯度下降并不特别“计算高效”。\n",
    "这暗示了两者之间可能有折中方案，这便涉及到*小批量随机梯度下降*（minibatch gradient descent）。\n",
    "\n",
    "## 向量化和缓存\n",
    "\n",
    "使用小批量的决策的核心是计算效率。\n",
    "当考虑与多个GPU和多台服务器并行处理时，这一点最容易被理解。在这种情况下，我们需要向每个GPU发送至少一张图像。\n",
    "有了每台服务器8个GPU和16台服务器，我们就能得到大小为128的小批量。\n",
    "\n",
    "当涉及到单个GPU甚至CPU时，事情会更微妙一些：\n",
    "这些设备有多种类型的内存、通常情况下多种类型的计算单元以及在它们之间不同的带宽限制。\n",
    "例如，一个CPU有少量寄存器（register），L1和L2缓存，以及L3缓存（在不同的处理器内核之间共享）。\n",
    "随着缓存的大小的增加，它们的延迟也在增加，同时带宽在减少。\n",
    "可以说，处理器能够执行的操作远比主内存接口所能提供的多得多。\n",
    "\n",
    "首先，具有16个内核和AVX-512向量化的2GHz CPU每秒可处理高达$2 \\cdot 10^9 \\cdot 16 \\cdot 32 = 10^{12}$个字节。\n",
    "同时，GPU的性能很容易超过该数字100倍。\n",
    "而另一方面，中端服务器处理器的带宽可能不超过100Gb/s，即不到处理器满负荷所需的十分之一。\n",
    "更糟糕的是，并非所有的内存入口都是相等的：内存接口通常为64位或更宽（例如，在最多384位的GPU上）。\n",
    "因此读取单个字节会导致由于更宽的存取而产生的代价。\n",
    "\n",
    "其次，第一次存取的额外开销很大，而按序存取（sequential access）或突发读取（burst read）相对开销较小。\n",
    "有关更深入的讨论，请参阅此[维基百科文章](https://en.wikipedia.org/wiki/Cache_hierarchy)。\n",
    "\n",
    "减轻这些限制的方法是使用足够快的CPU缓存层次结构来为处理器提供数据。\n",
    "这是深度学习中批量处理背后的推动力。\n",
    "举一个简单的例子：矩阵-矩阵乘法。\n",
    "比如$\\mathbf{A} = \\mathbf{B}\\mathbf{C}$，我们有很多方法来计算$\\mathbf{A}$。例如，我们可以尝试以下方法：\n",
    "\n",
    "1. 我们可以计算$\\mathbf{A}_{ij} = \\mathbf{B}_{i,:} \\mathbf{C}_{:,j}^\\top$，也就是说，我们可以通过点积进行逐元素计算。\n",
    "1. 我们可以计算$\\mathbf{A}_{:,j} = \\mathbf{B} \\mathbf{C}_{:,j}^\\top$，也就是说，我们可以一次计算一列。同样，我们可以一次计算$\\mathbf{A}$一行$\\mathbf{A}_{i,:}$。\n",
    "1. 我们可以简单地计算$\\mathbf{A} = \\mathbf{B} \\mathbf{C}$。\n",
    "1. 我们可以将$\\mathbf{B}$和$\\mathbf{C}$分成较小的区块矩阵，然后一次计算$\\mathbf{A}$的一个区块。\n",
    "\n",
    "如果我们使用第一个选择，每次我们计算一个元素$\\mathbf{A}_{ij}$时，都需要将一行和一列向量复制到CPU中。\n",
    "更糟糕的是，由于矩阵元素是按顺序对齐的，因此当从内存中读取它们时，我们需要访问两个向量中许多不相交的位置。\n",
    "第二种选择相对更有利：我们能够在遍历$\\mathbf{B}$的同时，将列向量$\\mathbf{C}_{:,j}$保留在CPU缓存中。\n",
    "它将内存带宽需求减半，相应地提高了访问速度。\n",
    "第三种选择表面上是最可取的，然而大多数矩阵可能不能完全放入缓存中。\n",
    "第四种选择提供了一个实践上很有用的方案：我们可以将矩阵的区块移到缓存中然后在本地将它们相乘。\n",
    "让我们来看看这些操作在实践中的效率如何。\n",
    "\n",
    "除了计算效率之外，Python和深度学习框架本身带来的额外开销也是相当大的。\n",
    "回想一下，每次我们执行代码时，Python解释器都会向深度学习框架发送一个命令，要求将其插入到计算图中并在调度过程中处理它。\n",
    "这样的额外开销可能是非常不利的。\n",
    "总而言之，我们最好用向量化（和矩阵）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3f047ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:30:13.593620Z",
     "iopub.status.busy": "2023-08-18T07:30:13.593044Z",
     "iopub.status.idle": "2023-08-18T07:30:17.706673Z",
     "shell.execute_reply": "2023-08-18T07:30:17.705758Z"
    },
    "origin_pos": 3,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from d2l import tensorflow as d2l\n",
    "\n",
    "timer = d2l.Timer()\n",
    "A = tf.Variable(tf.zeros((256, 256)))\n",
    "B = tf.Variable(tf.random.normal([256, 256], 0, 1))\n",
    "C = tf.Variable(tf.random.normal([256, 256], 0, 1))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "92eeb977",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "按元素分配只需遍历分别为$\\mathbf{B}$和$\\mathbf{C}$的所有行和列，即可将该值分配给$\\mathbf{A}$。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2eed16c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:30:17.712582Z",
     "iopub.status.busy": "2023-08-18T07:30:17.711888Z",
     "iopub.status.idle": "2023-08-18T07:32:18.357496Z",
     "shell.execute_reply": "2023-08-18T07:32:18.356665Z"
    },
    "origin_pos": 8,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "# 逐元素计算A=BC\n",
    "timer.start()\n",
    "for i in range(256):\n",
    "    for j in range(256):\n",
    "        A[i, j].assign(tf.tensordot(B[i, :], C[:, j], axes=1))\n",
    "timer.stop()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c56e1060",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "更快的策略是执行按列分配。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ebec8ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:32:18.361482Z",
     "iopub.status.busy": "2023-08-18T07:32:18.360861Z",
     "iopub.status.idle": "2023-08-18T07:32:18.732239Z",
     "shell.execute_reply": "2023-08-18T07:32:18.731369Z"
    },
    "origin_pos": 13,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "timer.start()\n",
    "for j in range(256):\n",
    "    A[:, j].assign(tf.tensordot(B, C[:, j], axes=1))\n",
    "timer.stop()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "10cc2b6f",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "最有效的方法是在一个区块中执行整个操作。让我们看看它们各自的操作速度是多少。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09e16e52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:32:18.736127Z",
     "iopub.status.busy": "2023-08-18T07:32:18.735494Z",
     "iopub.status.idle": "2023-08-18T07:32:18.764446Z",
     "shell.execute_reply": "2023-08-18T07:32:18.763584Z"
    },
    "origin_pos": 18,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "# 一次性计算A=BC\n",
    "timer.start()\n",
    "A.assign(tf.tensordot(B, C, axes=1))\n",
    "timer.stop()\n",
    "\n",
    "# 乘法和加法作为单独的操作（在实践中融合）\n",
    "gigaflops = [2/i for i in timer.times]\n",
    "print(f'performance in Gigaflops: element {gigaflops[0]:.3f}, '\n",
    "      f'column {gigaflops[1]:.3f}, full {gigaflops[2]:.3f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c056cc9c",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "## 小批量\n",
    "\n",
    ":label:`sec_minibatches`\n",
    "\n",
    "之前我们会理所当然地读取数据的*小批量*，而不是观测单个数据来更新参数，现在简要解释一下原因。\n",
    "处理单个观测值需要我们执行许多单一矩阵-矢量（甚至矢量-矢量）乘法，这耗费相当大，而且对应深度学习框架也要巨大的开销。\n",
    "这既适用于计算梯度以更新参数时，也适用于用神经网络预测。\n",
    "也就是说，每当我们执行$\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta_t \\mathbf{g}_t$时，消耗巨大。其中\n",
    "\n",
    "$$\\mathbf{g}_t = \\partial_{\\mathbf{w}} f(\\mathbf{x}_{t}, \\mathbf{w}).$$\n",
    "\n",
    "我们可以通过将其应用于一个小批量观测值来提高此操作的*计算*效率。\n",
    "也就是说，我们将梯度$\\mathbf{g}_t$替换为一个小批量而不是单个观测值\n",
    "\n",
    "$$\\mathbf{g}_t = \\partial_{\\mathbf{w}} \\frac{1}{|\\mathcal{B}_t|} \\sum_{i \\in \\mathcal{B}_t} f(\\mathbf{x}_{i}, \\mathbf{w}).$$\n",
    "\n",
    "让我们看看这对$\\mathbf{g}_t$的统计属性有什么影响：由于$\\mathbf{x}_t$和小批量$\\mathcal{B}_t$的所有元素都是从训练集中随机抽出的，因此梯度的期望保持不变。\n",
    "另一方面，方差显著降低。\n",
    "由于小批量梯度由正在被平均计算的$b := |\\mathcal{B}_t|$个独立梯度组成，其标准差降低了$b^{-\\frac{1}{2}}$。\n",
    "这本身就是一件好事，因为这意味着更新与完整的梯度更接近了。\n",
    "\n",
    "直观来说，这表明选择大型的小批量$\\mathcal{B}_t$将是普遍可行的。\n",
    "然而，经过一段时间后，与计算代价的线性增长相比，标准差的额外减少是微乎其微的。\n",
    "在实践中我们选择一个足够大的小批量，它可以提供良好的计算效率同时仍适合GPU的内存。\n",
    "下面，我们来看看这些高效的代码。\n",
    "在里面我们执行相同的矩阵-矩阵乘法，但是这次我们将其一次性分为64列的“小批量”。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce1b9df4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:32:18.768081Z",
     "iopub.status.busy": "2023-08-18T07:32:18.767493Z",
     "iopub.status.idle": "2023-08-18T07:32:18.779510Z",
     "shell.execute_reply": "2023-08-18T07:32:18.778717Z"
    },
    "origin_pos": 23,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "timer.start()\n",
    "for j in range(0, 256, 64):\n",
    "    A[:, j:j+64].assign(tf.tensordot(B, C[:, j:j+64], axes=1))\n",
    "timer.stop()\n",
    "print(f'performance in Gigaflops: block {2 / timer.times[3]:.3f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b0ff4769",
   "metadata": {
    "origin_pos": 25
   },
   "source": [
    "显而易见，小批量上的计算基本上与完整矩阵一样有效。\n",
    "需要注意的是，在 :numref:`sec_batch_norm`中，我们使用了一种在很大程度上取决于小批量中的方差的正则化。\n",
    "随着后者增加，方差会减少，随之而来的是批量规范化带来的噪声注入的好处。\n",
    "关于实例，请参阅 :cite:`Ioffe.2017`，了解有关如何重新缩放并计算适当项目。\n",
    "\n",
    "## 读取数据集\n",
    "\n",
    "让我们来看看如何从数据中有效地生成小批量。\n",
    "下面我们使用NASA开发的测试机翼的数据集[不同飞行器产生的噪声](https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise)来比较这些优化算法。\n",
    "为方便起见，我们只使用前$1,500$样本。\n",
    "数据已作预处理：我们移除了均值并将方差重新缩放到每个坐标为$1$。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ace5a948",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:32:18.782972Z",
     "iopub.status.busy": "2023-08-18T07:32:18.782444Z",
     "iopub.status.idle": "2023-08-18T07:32:18.787908Z",
     "shell.execute_reply": "2023-08-18T07:32:18.787047Z"
    },
    "origin_pos": 28,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat',\n",
    "                           '76e5be1548fd8222e5074cf0faae75edff8cf93f')\n",
    "\n",
    "#@save\n",
    "def get_data_ch11(batch_size=10, n=1500):\n",
    "    data = np.genfromtxt(d2l.download('airfoil'),\n",
    "                         dtype=np.float32, delimiter='\\t')\n",
    "    data = (data - data.mean(axis=0)) / data.std(axis=0)\n",
    "    data_iter = d2l.load_array((data[:n, :-1], data[:n, -1]),\n",
    "                               batch_size, is_train=True)\n",
    "    return data_iter, data.shape[1]-1"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "db2e2e30",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "## 从零开始实现\n",
    "\n",
    " :numref:`sec_linear_scratch`一节中已经实现过小批量随机梯度下降算法。\n",
    "我们在这里将它的输入参数变得更加通用，主要是为了方便本章后面介绍的其他优化算法也可以使用同样的输入。\n",
    "具体来说，我们添加了一个状态输入`states`并将超参数放在字典`hyperparams`中。\n",
    "此外，我们将在训练函数里对各个小批量样本的损失求平均，因此优化算法中的梯度不需要除以批量大小。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c070bff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:32:18.791328Z",
     "iopub.status.busy": "2023-08-18T07:32:18.790806Z",
     "iopub.status.idle": "2023-08-18T07:32:18.794894Z",
     "shell.execute_reply": "2023-08-18T07:32:18.794105Z"
    },
    "origin_pos": 33,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "def sgd(params, grads, states, hyperparams):\n",
    "    for param, grad in zip(params, grads):\n",
    "        param.assign_sub(hyperparams['lr']*grad)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d09efb5a",
   "metadata": {
    "origin_pos": 35
   },
   "source": [
    "下面实现一个通用的训练函数，以方便本章后面介绍的其他优化算法使用。\n",
    "它初始化了一个线性回归模型，然后可以使用小批量随机梯度下降以及后续小节介绍的其他算法来训练模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83e7290c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:32:18.798562Z",
     "iopub.status.busy": "2023-08-18T07:32:18.797932Z",
     "iopub.status.idle": "2023-08-18T07:32:18.806294Z",
     "shell.execute_reply": "2023-08-18T07:32:18.805501Z"
    },
    "origin_pos": 38,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "#@save\n",
    "def train_ch11(trainer_fn, states, hyperparams, data_iter,\n",
    "               feature_dim, num_epochs=2):\n",
    "    # 初始化模型\n",
    "    w = tf.Variable(tf.random.normal(shape=(feature_dim, 1),\n",
    "                                   mean=0, stddev=0.01),trainable=True)\n",
    "    b = tf.Variable(tf.zeros(1), trainable=True)\n",
    "\n",
    "    # 训练模型\n",
    "    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[0, num_epochs], ylim=[0.22, 0.35])\n",
    "    n, timer = 0, d2l.Timer()\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        for X, y in data_iter:\n",
    "          with tf.GradientTape() as g:\n",
    "            l = tf.math.reduce_mean(loss(net(X), y))\n",
    "\n",
    "          dw, db = g.gradient(l, [w, b])\n",
    "          trainer_fn([w, b], [dw, db], states, hyperparams)\n",
    "          n += X.shape[0]\n",
    "          if n % 200 == 0:\n",
    "              timer.stop()\n",
    "              p = n/X.shape[0]\n",
    "              q = p/tf.data.experimental.cardinality(data_iter).numpy()\n",
    "              r = (d2l.evaluate_loss(net, data_iter, loss),)\n",
    "              animator.add(q, r)\n",
    "              timer.start()\n",
    "    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')\n",
    "    return timer.cumsum(), animator.Y[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5b6e7427",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "让我们来看看批量梯度下降的优化是如何进行的。\n",
    "这可以通过将小批量设置为1500（即样本总数）来实现。\n",
    "因此，模型参数每个迭代轮数只迭代一次。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ba4552b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:32:18.809965Z",
     "iopub.status.busy": "2023-08-18T07:32:18.809312Z",
     "iopub.status.idle": "2023-08-18T07:32:19.886126Z",
     "shell.execute_reply": "2023-08-18T07:32:19.885262Z"
    },
    "origin_pos": 41,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "def train_sgd(lr, batch_size, num_epochs=2):\n",
    "    data_iter, feature_dim = get_data_ch11(batch_size)\n",
    "    return train_ch11(\n",
    "        sgd, None, {'lr': lr}, data_iter, feature_dim, num_epochs)\n",
    "\n",
    "gd_res = train_sgd(1, 1500, 10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5a549401",
   "metadata": {
    "origin_pos": 42
   },
   "source": [
    "当批量大小为1时，优化使用的是随机梯度下降。\n",
    "为了简化实现，我们选择了很小的学习率。\n",
    "在随机梯度下降的实验中，每当一个样本被处理，模型参数都会更新。\n",
    "在这个例子中，这相当于每个迭代轮数有1500次更新。\n",
    "可以看到，目标函数值的下降在1个迭代轮数后就变得较为平缓。\n",
    "尽管两个例子在一个迭代轮数内都处理了1500个样本，但实验中随机梯度下降的一个迭代轮数耗时更多。\n",
    "这是因为随机梯度下降更频繁地更新了参数，而且一次处理单个观测值效率较低。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe4f334e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:32:19.890097Z",
     "iopub.status.busy": "2023-08-18T07:32:19.889514Z",
     "iopub.status.idle": "2023-08-18T07:32:52.588628Z",
     "shell.execute_reply": "2023-08-18T07:32:52.587704Z"
    },
    "origin_pos": 43,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "sgd_res = train_sgd(0.005, 1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "249a25b4",
   "metadata": {
    "origin_pos": 44
   },
   "source": [
    "最后，当批量大小等于100时，我们使用小批量随机梯度下降进行优化。\n",
    "每个迭代轮数所需的时间比随机梯度下降和批量梯度下降所需的时间短。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "928ef76b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:32:52.593840Z",
     "iopub.status.busy": "2023-08-18T07:32:52.593190Z",
     "iopub.status.idle": "2023-08-18T07:32:55.146174Z",
     "shell.execute_reply": "2023-08-18T07:32:55.145304Z"
    },
    "origin_pos": 45,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "mini1_res = train_sgd(.4, 100)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9268e552",
   "metadata": {
    "origin_pos": 46
   },
   "source": [
    "将批量大小减少到10，每个迭代轮数的时间都会增加，因为每批工作负载的执行效率变得更低。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23037d12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:32:55.150065Z",
     "iopub.status.busy": "2023-08-18T07:32:55.149451Z",
     "iopub.status.idle": "2023-08-18T07:33:00.469733Z",
     "shell.execute_reply": "2023-08-18T07:33:00.468844Z"
    },
    "origin_pos": 47,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "mini2_res = train_sgd(.05, 10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4c361d72",
   "metadata": {
    "origin_pos": 48
   },
   "source": [
    "现在我们可以比较前四个实验的时间与损失。\n",
    "可以看出，尽管在处理的样本数方面，随机梯度下降的收敛速度快于梯度下降，但与梯度下降相比，它需要更多的时间来达到同样的损失，因为逐个样本来计算梯度并不那么有效。\n",
    "小批量随机梯度下降能够平衡收敛速度和计算效率。\n",
    "大小为10的小批量比随机梯度下降更有效；\n",
    "大小为100的小批量在运行时间上甚至优于梯度下降。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccc49575",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:33:00.473590Z",
     "iopub.status.busy": "2023-08-18T07:33:00.472973Z",
     "iopub.status.idle": "2023-08-18T07:33:01.061588Z",
     "shell.execute_reply": "2023-08-18T07:33:01.060720Z"
    },
    "origin_pos": 49,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "d2l.set_figsize([6, 3])\n",
    "d2l.plot(*list(map(list, zip(gd_res, sgd_res, mini1_res, mini2_res))),\n",
    "         'time (sec)', 'loss', xlim=[1e-2, 10],\n",
    "         legend=['gd', 'sgd', 'batch size=100', 'batch size=10'])\n",
    "d2l.plt.gca().set_xscale('log')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "91983414",
   "metadata": {
    "origin_pos": 50
   },
   "source": [
    "## 简洁实现\n",
    "\n",
    "下面用深度学习框架自带算法实现一个通用的训练函数，我们将在本章中其它小节使用它。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c5699e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:33:01.065577Z",
     "iopub.status.busy": "2023-08-18T07:33:01.064969Z",
     "iopub.status.idle": "2023-08-18T07:33:01.073202Z",
     "shell.execute_reply": "2023-08-18T07:33:01.072411Z"
    },
    "origin_pos": 53,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "#@save\n",
    "def train_concise_ch11(trainer_fn, hyperparams, data_iter, num_epochs=2):\n",
    "    # 初始化模型\n",
    "    net = tf.keras.Sequential()\n",
    "    net.add(tf.keras.layers.Dense(1,\n",
    "            kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n",
    "    optimizer = trainer_fn(**hyperparams)\n",
    "    loss = tf.keras.losses.MeanSquaredError()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[0, num_epochs], ylim=[0.22, 0.35])\n",
    "    n, timer = 0, d2l.Timer()\n",
    "    for _ in range(num_epochs):\n",
    "        for X, y in data_iter:\n",
    "            with tf.GradientTape() as g:\n",
    "                out = net(X)\n",
    "                l = loss(y, out)\n",
    "                params = net.trainable_variables\n",
    "                grads = g.gradient(l, params)\n",
    "            optimizer.apply_gradients(zip(grads, params))\n",
    "            n += X.shape[0]\n",
    "            if n % 200 == 0:\n",
    "                timer.stop()\n",
    "                p = n/X.shape[0]\n",
    "                q = p/tf.data.experimental.cardinality(data_iter).numpy()\n",
    "                # MeanSquaredError计算平方误差时不带系数1/2\n",
    "                r = (d2l.evaluate_loss(net, data_iter, loss) / 2,)\n",
    "                animator.add(q, r)\n",
    "                timer.start()\n",
    "    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6bd3b9a3",
   "metadata": {
    "origin_pos": 55
   },
   "source": [
    "下面使用这个训练函数，复现之前的实验。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60cb8864",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:33:01.076719Z",
     "iopub.status.busy": "2023-08-18T07:33:01.076158Z",
     "iopub.status.idle": "2023-08-18T07:33:09.348623Z",
     "shell.execute_reply": "2023-08-18T07:33:09.347640Z"
    },
    "origin_pos": 58,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "data_iter, _ = get_data_ch11(10)\n",
    "trainer = tf.keras.optimizers.SGD\n",
    "train_concise_ch11(trainer, {'learning_rate': 0.05}, data_iter)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "34f79df6",
   "metadata": {
    "origin_pos": 60
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 由于减少了深度学习框架的额外开销，使用更好的内存定位以及CPU和GPU上的缓存，向量化使代码更加高效。\n",
    "* 随机梯度下降的“统计效率”与大批量一次处理数据的“计算效率”之间存在权衡。小批量随机梯度下降提供了两全其美的答案：计算和统计效率。\n",
    "* 在小批量随机梯度下降中，我们处理通过训练数据的随机排列获得的批量数据（即每个观测值只处理一次，但按随机顺序）。\n",
    "* 在训练期间降低学习率有助于训练。\n",
    "* 一般来说，小批量随机梯度下降比随机梯度下降和梯度下降的速度快，收敛风险较小。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 修改批量大小和学习率，并观察目标函数值的下降率以及每个迭代轮数消耗的时间。\n",
    "1. 将小批量随机梯度下降与实际从训练集中*取样替换*的变体进行比较。会看出什么？\n",
    "1. 一个邪恶的精灵在没通知你的情况下复制了你的数据集（即每个观测发生两次，数据集增加到原始大小的两倍，但没有人告诉你）。随机梯度下降、小批量随机梯度下降和梯度下降的表现将如何变化？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07bb1b4",
   "metadata": {
    "origin_pos": 63,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/4326)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
