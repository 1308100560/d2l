{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3ebed1e",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 多GPU的简洁实现\n",
    ":label:`sec_multi_gpu_concise`\n",
    "\n",
    "每个新模型的并行计算都从零开始实现是无趣的。此外，优化同步工具以获得高性能也是有好处的。下面我们将展示如何使用深度学习框架的高级API来实现这一点。数学和算法与 :numref:`sec_multi_gpu`中的相同。本节的代码至少需要两个GPU来运行。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11bd98b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:14:31.691978Z",
     "iopub.status.busy": "2023-08-18T07:14:31.691452Z",
     "iopub.status.idle": "2023-08-18T07:14:36.006544Z",
     "shell.execute_reply": "2023-08-18T07:14:36.005514Z"
    },
    "origin_pos": 1,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "from mxnet import autograd, gluon, init, np, npx\n",
    "from mxnet.gluon import nn\n",
    "from d2l import mxnet as d2l\n",
    "\n",
    "npx.set_np()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "67ea9200",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## [**简单网络**]\n",
    "\n",
    "让我们使用一个比 :numref:`sec_multi_gpu`的LeNet更有意义的网络，它依然能够容易地和快速地训练。我们选择的是 :cite:`He.Zhang.Ren.ea.2016`中的ResNet-18。因为输入的图像很小，所以稍微修改了一下。与 :numref:`sec_resnet`的区别在于，我们在开始时使用了更小的卷积核、步长和填充，而且删除了最大汇聚层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb5ef6a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:14:36.011118Z",
     "iopub.status.busy": "2023-08-18T07:14:36.010554Z",
     "iopub.status.idle": "2023-08-18T07:14:36.018193Z",
     "shell.execute_reply": "2023-08-18T07:14:36.017296Z"
    },
    "origin_pos": 5,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "#@save\n",
    "def resnet18(num_classes):\n",
    "    \"\"\"稍加修改的ResNet-18模型\"\"\"\n",
    "    def resnet_block(num_channels, num_residuals, first_block=False):\n",
    "        blk = nn.Sequential()\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.add(d2l.Residual(\n",
    "                    num_channels, use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.add(d2l.Residual(num_channels))\n",
    "        return blk\n",
    "\n",
    "    net = nn.Sequential()\n",
    "    # 该模型使用了更小的卷积核、步长和填充，而且删除了最大汇聚层\n",
    "    net.add(nn.Conv2D(64, kernel_size=3, strides=1, padding=1),\n",
    "            nn.BatchNorm(), nn.Activation('relu'))\n",
    "    net.add(resnet_block(64, 2, first_block=True),\n",
    "            resnet_block(128, 2),\n",
    "            resnet_block(256, 2),\n",
    "            resnet_block(512, 2))\n",
    "    net.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes))\n",
    "    return net"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d958771d",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "## 网络初始化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91490809",
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "`initialize`函数允许我们在所选设备上初始化参数。请参阅 :numref:`sec_numerical_stability`复习初始化方法。这个函数在多个设备上初始化网络时特别方便。下面在实践中试一试它的运作方式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c14eaf36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:14:36.022097Z",
     "iopub.status.busy": "2023-08-18T07:14:36.021549Z",
     "iopub.status.idle": "2023-08-18T07:14:37.098412Z",
     "shell.execute_reply": "2023-08-18T07:14:37.097505Z"
    },
    "origin_pos": 11,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "net = resnet18(10)\n",
    "# 获取GPU列表\n",
    "devices = d2l.try_all_gpus()\n",
    "# 初始化网络的所有参数\n",
    "net.initialize(init=init.Normal(sigma=0.01), ctx=devices)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ee7154a1",
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "使用 :numref:`sec_multi_gpu`中引入的`split_and_load`函数可以切分一个小批量数据，并将切分后的分块数据复制到`devices`变量提供的设备列表中。网络实例自动使用适当的GPU来计算前向传播的值。我们将在下面生成$4$个观测值，并在GPU上将它们拆分。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bab94de0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:14:37.101727Z",
     "iopub.status.busy": "2023-08-18T07:14:37.101430Z",
     "iopub.status.idle": "2023-08-18T07:14:38.931458Z",
     "shell.execute_reply": "2023-08-18T07:14:38.930470Z"
    },
    "origin_pos": 15,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "x = np.random.uniform(size=(4, 1, 28, 28))\n",
    "x_shards = gluon.utils.split_and_load(x, devices)\n",
    "net(x_shards[0]), net(x_shards[1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "843a056e",
   "metadata": {
    "origin_pos": 16,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "一旦数据通过网络，网络对应的参数就会在*有数据通过的设备上初始化*。这意味着初始化是基于每个设备进行的。由于我们选择的是GPU0和GPU1，所以网络只在这两个GPU上初始化，而不是在CPU上初始化。事实上，CPU上甚至没有这些参数。我们可以通过打印参数和观察可能出现的任何错误来验证这一点。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6245fb1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:14:38.935941Z",
     "iopub.status.busy": "2023-08-18T07:14:38.935108Z",
     "iopub.status.idle": "2023-08-18T07:14:38.945047Z",
     "shell.execute_reply": "2023-08-18T07:14:38.944245Z"
    },
    "origin_pos": 17,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "weight = net[0].params.get('weight')\n",
    "\n",
    "try:\n",
    "    weight.data()\n",
    "except RuntimeError:\n",
    "    print('not initialized on cpu')\n",
    "weight.data(devices[0])[0], weight.data(devices[1])[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "77f162a0",
   "metadata": {
    "origin_pos": 18,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "接下来，让我们使用[**在多个设备上并行工作**]的代码来替换前面的[**评估模型**]的代码。\n",
    "这里主要是 :numref:`sec_lenet`的`evaluate_accuracy_gpu`函数的替代，代码的主要区别在于在调用网络之前拆分了一个小批量，其他在本质上是一样的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce86921c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:14:38.948592Z",
     "iopub.status.busy": "2023-08-18T07:14:38.948314Z",
     "iopub.status.idle": "2023-08-18T07:14:38.954390Z",
     "shell.execute_reply": "2023-08-18T07:14:38.953585Z"
    },
    "origin_pos": 19,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "#@save\n",
    "def evaluate_accuracy_gpus(net, data_iter, split_f=d2l.split_batch):\n",
    "    \"\"\"使用多个GPU计算数据集上模型的精度\"\"\"\n",
    "    # 查询设备列表\n",
    "    devices = list(net.collect_params().values())[0].list_ctx()\n",
    "    # 正确预测的数量，预测的总数量\n",
    "    metric = d2l.Accumulator(2)\n",
    "    for features, labels in data_iter:\n",
    "        X_shards, y_shards = split_f(features, labels, devices)\n",
    "        # 并行运行\n",
    "        pred_shards = [net(X_shard) for X_shard in X_shards]\n",
    "        metric.add(sum(float(d2l.accuracy(pred_shard, y_shard)) for\n",
    "                       pred_shard, y_shard in zip(\n",
    "                           pred_shards, y_shards)), labels.size)\n",
    "    return metric[0] / metric[1]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fb429758",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "## [**训练**]\n",
    "\n",
    "如前所述，用于训练的代码需要执行几个基本功能才能实现高效并行：\n",
    "\n",
    "* 需要在所有设备上初始化网络参数；\n",
    "* 在数据集上迭代时，要将小批量数据分配到所有设备上；\n",
    "* 跨设备并行计算损失及其梯度；\n",
    "* 聚合梯度，并相应地更新参数。\n",
    "\n",
    "最后，并行地计算精确度和发布网络的最终性能。除了需要拆分和聚合数据外，训练代码与前几章的实现非常相似。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00e4e7ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:14:38.958189Z",
     "iopub.status.busy": "2023-08-18T07:14:38.957632Z",
     "iopub.status.idle": "2023-08-18T07:14:38.965251Z",
     "shell.execute_reply": "2023-08-18T07:14:38.964467Z"
    },
    "origin_pos": 21,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "def train(num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "    ctx = [d2l.try_gpu(i) for i in range(num_gpus)]\n",
    "    net.initialize(init=init.Normal(sigma=0.01), ctx=ctx, force_reinit=True)\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd',\n",
    "                            {'learning_rate': lr})\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    timer, num_epochs = d2l.Timer(), 10\n",
    "    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        timer.start()\n",
    "        for features, labels in train_iter:\n",
    "            X_shards, y_shards = d2l.split_batch(features, labels, ctx)\n",
    "            with autograd.record():\n",
    "                ls = [loss(net(X_shard), y_shard) for X_shard, y_shard\n",
    "                      in zip(X_shards, y_shards)]\n",
    "            for l in ls:\n",
    "                l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        npx.waitall()\n",
    "        timer.stop()\n",
    "        animator.add(epoch + 1, (evaluate_accuracy_gpus(net, test_iter),))\n",
    "    print(f'测试精度：{animator.Y[0][-1]:.2f}，{timer.avg():.1f}秒/轮，'\n",
    "          f'在{str(ctx)}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "97e3d506",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "接下来看看这在实践中是如何运作的。我们先[**在单个GPU上训练网络**]进行预热。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2edf135",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:14:38.969001Z",
     "iopub.status.busy": "2023-08-18T07:14:38.968454Z",
     "iopub.status.idle": "2023-08-18T07:17:25.276863Z",
     "shell.execute_reply": "2023-08-18T07:17:25.275549Z"
    },
    "origin_pos": 25,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "train(num_gpus=1, batch_size=256, lr=0.1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4f533a87",
   "metadata": {
    "origin_pos": 27
   },
   "source": [
    "接下来我们[**使用2个GPU进行训练**]。与 :numref:`sec_multi_gpu`中评估的LeNet相比，ResNet-18的模型要复杂得多。这就是显示并行化优势的地方，计算所需时间明显大于同步参数需要的时间。因为并行化开销的相关性较小，因此这种操作提高了模型的可伸缩性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ac0758b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:17:25.281316Z",
     "iopub.status.busy": "2023-08-18T07:17:25.280388Z",
     "iopub.status.idle": "2023-08-18T07:18:49.899939Z",
     "shell.execute_reply": "2023-08-18T07:18:49.898975Z"
    },
    "origin_pos": 28,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "train(num_gpus=2, batch_size=512, lr=0.2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1a7ed1be",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "## 小结\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e78f48",
   "metadata": {
    "origin_pos": 31,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "* Gluon通过提供一个上下文列表，为跨多个设备的模型初始化提供原语。\n",
    "* 神经网络可以在（可找到数据的）单GPU上进行自动评估。\n",
    "* 每台设备上的网络需要先初始化，然后再尝试访问该设备上的参数，否则会遇到错误。\n",
    "* 优化算法在多个GPU上自动聚合。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4d4f36",
   "metadata": {
    "origin_pos": 33
   },
   "source": [
    "## 练习\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c48ddf",
   "metadata": {
    "origin_pos": 34,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "1. 本节使用ResNet-18，请尝试不同的迭代周期数、批量大小和学习率，以及使用更多的GPU进行计算。如果使用$16$个GPU（例如，在AWS p2.16xlarge实例上）尝试此操作，会发生什么？\n",
    "1. 有时候不同的设备提供了不同的计算能力，我们可以同时使用GPU和CPU，那应该如何分配工作？为什么？\n",
    "1. 如果去掉`npx.waitall()`会怎样？该如何修改训练，以使并行操作最多有两个步骤重叠？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2076dba0",
   "metadata": {
    "origin_pos": 36,
    "tab": [
     "mxnet"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/2804)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
